tool.class = org.hypertrace.core.viewcreator.pinot.PinotTableCreationTool
service.name = "sample-view"

view = {
  name = myView
  output.schema.class = org.hypertrace.core.viewcreator.test.api.TestView
  output.schema.url = "host:port/myView"
}

pinot = {
  controllerHost = localhost
  controllerPort = 9000
  sortedColumn = "tenant_id"
  timeColumn = creation_time_millis
  timeUnit = MILLISECONDS
  dimensionColumns = [name, creation_time_millis, id_sha, friends, properties__KEYS, properties__VALUES]
  dateTimeColumns = [start_time_millis, end_time_millis, bucket_start_time_millis]
  columnsMaxLength = {id_sha: 64}
  metricColumns = [time_taken_millis]
  invertedIndexColumns = [friends, properties__KEYS, properties__VALUES]
  noDictionaryColumns = [properties__VALUES]
  bloomFilterColumns = [id_sha]
  rangeIndexColumns = [creation_time_millis, start_time_millis]
  tableName = myView1
  loadMode = MMAP
  numReplicas = 2
  retentionTimeValue = 3
  retentionTimeUnit = DAYS
  brokerTenant = defaultBroker
  serverTenant = defaultServer
  segmentAssignmentStrategy = BalanceNumSegmentAssignmentStrategy
  peerSegmentDownloadScheme = http
}

pinotRealtime = {
  replicasPerPartition = 1
  streamConfigs = {
    streamType: kafka,
    stream.kafka.consumer.type: LowLevel,
    stream.kafka.topic.name: test-view-events,
    stream.kafka.consumer.factory.class.name: "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
    stream.kafka.decoder.class.name: "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",
    stream.kafka.decoder.prop.schema.registry.rest.url: "http://localhost:8081",
    stream.kafka.hlc.zk.connect.string: "localhost:2181",
    stream.kafka.zk.broker.url: "localhost:2181",
    stream.kafka.broker.list: "localhost:9092",
    realtime.segment.flush.threshold.time: 3600000,
    realtime.segment.flush.threshold.size: 500000,
    stream.kafka.consumer.prop.auto.offset.reset: largest
  }

  taskConfigs = {
    RealtimeToOfflineSegmentsTask = {
      bucketTimePeriod = "6h"
      bufferTimePeriod = "12h"
    }
  }

  transformConfigs = [{
    columnName = "bucket_start_time_millis"
    transformFunction = "round(start_time_millis, 3600000)"
  }]

  tagOverrideConfigs = {
    realtimeConsuming = "tier-for-consuming"
    realtimeCompleted = "tier-for-completed"
  }

  tierConfigs = [{
    name = "hot-data-tier"
    segmentSelectorType = "time"
    segmentAge = "5d"
    storageType = "pinot_server"
    serverTag = "tier-for-hot-data"
  }]

  starTreeIndexConfigs = [{
    dimensionsSplitOrder = [
      "tenant_id",
      "start_time_millis"
    ],
    skipStarNodeCreationForDimensions = [],
    functionColumnPairs = [
      "COUNT__name",
      "SUM__time_taken_millis"
    ],
    maxLeafRecords = 100
  }]

  segmentPartitionConfig = {
    columnPartitionMap = {
      tenant_id = {
        functionName = "Modulo",
        numPartitions = 4
      }
    }
  }

  routingConfig = {
    instanceSelectorType = "replicaGroup"
    segmentPrunerTypes =  ["time", "partition"]
  }
}

pinotOffline = {
  retentionTimeValue = 90
  retentionTimeUnit = DAYS
}
